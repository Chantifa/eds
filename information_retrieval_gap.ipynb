{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This jupyter notebook contains some gaps indicated by \"*Your code*\". Your tasks is to fill these gaps with appropriate python code. \n",
    "\n",
    "Also try to answer the following questions?\n",
    "* With which measure (jaccard or cosine) do you obtain larger values? \n",
    "* Which is missing in the current system, which is usually part of real-world information retrieval systems? \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv,sys,math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read idf weights from file\n",
    "def read_idf_weights(idf_filename,map_idf):\n",
    "    max_idf=-1\n",
    "    fin=open(idf_filename)\n",
    "    for line in fin:\n",
    "        line=line.strip()\n",
    "        i=line.find(\":\")\n",
    "        word=line[0:i]\n",
    "        idf=float(line[i+1:len(line)])\n",
    "        map_idf[word]=idf\n",
    "        max_idf=max(max_idf,idf)\n",
    "    return max_idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use union and intersections\n",
    "## s1: set of words of document 1\n",
    "## s2: set of words of document 2\n",
    "def calculate_jaccard(s1,s2):\n",
    "   ## YOUR CODE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the scalar product between two tfidf vectors represented in a sparse format\n",
    "# map_tf1: maps a word to its term frequence in document 1\n",
    "# map_tf2: maps a word to its term frequence in document 2\n",
    "# map_idf: maps a word to log of its inverse document frequence\n",
    "# max_idf: maximum total value of logarithm of inverse document frequence\n",
    "def scalar_product(map_tf1,map_tf2,map_idf,max_idf):\n",
    "    valall=0\n",
    "    for key,tf1 in map_tf1.items():\n",
    "        idf=map_idf.get(key) ## note idf already CONTAINS the logarithm!\n",
    "        if idf==None:\n",
    "            idf=max_idf\n",
    "        tf2=map_tf2.get(key)\n",
    "        if tf2!=None:\n",
    "            valall+=# YOUR CODE multiply the two tf-idf values\n",
    "    valall=math.sqrt(valall)\n",
    "    return valall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## define in terms of scalar product\n",
    "## map_tf1: maps a word in document 1 to its term frequence\n",
    "## map_tf2: maps a word in document 2 to its term frequence\n",
    "## map_idf: maps a word to its logarithm of inverse document frequence\n",
    "## max_idf: maximum value of logarithm of inverse document frequence\n",
    "def calc_cosine_measure(map_tf1,map_tf2,map_idf,max_idf):\n",
    "   ## Your code -> resort to scalar product, the method above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read stop words from file\n",
    "## file where the stop words come from\n",
    "def read_stop_words(filename):\n",
    "    stop_words=[]\n",
    "    with open(filename) as fin:\n",
    "        for line in fin:\n",
    "            line=line.strip()\n",
    "            stop_words.append(line)\n",
    "    return stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize string\n",
    "def tokenize(s,ssep):\n",
    "    tokens=[]\n",
    "    sout=\"\"\n",
    "    for c in s:\n",
    "        if ssep.find(c)!=-1:\n",
    "            if sout!=\"\":\n",
    "                tokens.append(sout)\n",
    "                sout=\"\"\n",
    "        else:\n",
    "            sout+=c\n",
    "    if sout!=\"\":\n",
    "        tokens.append(sout)\n",
    "        sout=\"\"\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if key exists, increase its count by one\n",
    "### otherwise set its count to 1\n",
    "### ht: hashtable that mapps a word to its number of occurrences in a document (term frequence)\n",
    "def putcount(ht,key):\n",
    "    val=ht.get(key)\n",
    "    if val==None:\n",
    "        ht[key]=1\n",
    "    else:\n",
    "        ht[key]=val+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_texts(input_filename,stop_word_filename):\n",
    "    entries=[]\n",
    "    stop_words=[]\n",
    "\n",
    "    if stop_word_filename!=None:\n",
    "        stop_words=read_stop_words(stop_word_filename)\n",
    "\n",
    "    with open(input_filename,encoding=\"utf-8\") as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=';',  quotechar='\\\"')\n",
    "        line_count = 0\n",
    "        first_time=True\n",
    "        for row in csv_reader:\n",
    "            if first_time:\n",
    "                first_time=None\n",
    "                continue\n",
    "            map_count={}\n",
    "            words=set()\n",
    "\n",
    "            if len(row)<2:\n",
    "                continue\n",
    "            description=row[1]\n",
    "            title=row[0]\n",
    "            tokens=tokenize(description,\"!? .,;)(\")\n",
    "            for token in tokens:\n",
    "                token=token.strip()\n",
    "                if token in stop_words:\n",
    "                    continue\n",
    "                putcount(map_count,token)\n",
    "                words.add(token)\n",
    "            entry={}\n",
    "            entry[\"title\"]=title\n",
    "            entry[\"description\"]=description\n",
    "            entry[\"words\"]=words\n",
    "            entry[\"counts\"]=map_count\n",
    "            entries.append(entry)\n",
    "    return entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add the best n results to the list\n",
    "def add_n_best(li_results,result_entry,maxlen):\n",
    "    inserted=None\n",
    "    for i in range(0,len(li_results),1):\n",
    "        new_score=result_entry[\"score\"]\n",
    "        score_li=li_results[i][\"score\"]\n",
    "        if new_score>score_li:\n",
    "            li_results.insert(i,result_entry)\n",
    "            inserted=True\n",
    "            break\n",
    "    if not inserted and len(li_results)<maxlen:\n",
    "        li_results.append(result_entry)\n",
    "        return\n",
    "    if len(li_results)>maxlen:\n",
    "        del(li_results[len(li_results)-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_best_match(search_expression,entries,search_method,map_idf,max_idf):\n",
    "    li_results=[]\n",
    "    tokens_search_expression=tokenize(search_expression,\" !()?\")\n",
    "    map_counts1={}\n",
    "    for token in tokens_search_expression:\n",
    "        putcount(map_counts1,token)\n",
    "\n",
    "    set_search_expression=set(tokens_search_expression)\n",
    "    best_score=-1\n",
    "    best_entry=[]\n",
    "    for entry in entries:\n",
    "        words=entry[\"words\"]\n",
    "        map_counts2=entry[\"counts\"]\n",
    "        if search_method==\"jaccard\":\n",
    "            score=calculate_jaccard(set_search_expression,words)\n",
    "        else:\n",
    "            score=calc_cosine_measure(map_counts1,map_counts2,map_idf,max_idf)\n",
    "        new_entry={\"entry\": entry,\"score\":score}\n",
    "        add_n_best(li_results,new_entry,5)\n",
    "    return li_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load the idf weighting table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_idf:  19.2454\n",
      "map_idf:  {'Abend': 6.11366, 'AbenddÃ¤mmerung': 10.7859, 'Abendesse': 9.24113, 'Abscheu': 9.52425, 'Absicht': \n"
     ]
    }
   ],
   "source": [
    "idf_weights_filename=\"weights.txt\"\n",
    "map_idf={}\n",
    "max_idf=read_idf_weights(idf_weights_filename,map_idf)\n",
    "print (\"max_idf: \",max_idf)\n",
    "print (\"map_idf: \",str(map_idf)[0:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a search method and specify the words you want to search for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_expression=\"Linux Software\"\n",
    "search_method=\"cosine\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Komponentenbasierte Entwicklung Eingebetteter Systeme (KEES)\n",
      "Der ständige Preisverfall und die Leistungssteigerung der Hardwarekomponenten, vor allem der Mikroprozessoren und Speicher, erschließen ständig neue E\n",
      "0.5245714166210287\n",
      "--------------------------\n",
      "Komponentenbasierte Entwicklung Eingebetteter Systeme (KEES)\n",
      "Der ständige Preisverfall und die Leistungssteigerung der Hardwarekomponenten, vor allem der Mikroprozessoren und Speicher, erschließen ständig neue E\n",
      "0.5245714166210287\n",
      "--------------------------\n",
      "Validierung komponentenbasierter Software für Echtzeitsysteme\n",
      "Der Anteil und die Bedeutung von Software bei der Entwicklung von Echtzeitsystemen wächst sehr stark. Gleichzeitig werden die Entwicklungszyklen für S\n",
      "0.5180160164935071\n",
      "--------------------------\n",
      "Entwicklung eines webbasierten Systems zur Erfassung und Bearbeitung von Modification Requests\n",
      "Nach der Inbetriebnahme einer Software gibt es vielfältige Gründe, die zu\n",
      "Problemen mit der Software bei den Anwendern führen. Der Anwender meldet \n",
      "se\n",
      "0.49299629927536015\n",
      "--------------------------\n",
      "Validierung komponentenbasierter Software für verteilte Echtzeitsysteme\n",
      "Im vorliegenden Beitrag wird eine Einführung in die Eigenschaften\n",
      "komponentenbasierter Software gegeben. Es wird ein Verfahren zur\n",
      "Validierung kompone\n",
      "0.48587374609089495\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "entries=index_texts(\"texts.csv\",None)\n",
    "best_matches=search_best_match(search_expression,entries,search_method,map_idf,max_idf)\n",
    "for best_match in best_matches:\n",
    "    entry=best_match[\"entry\"]\n",
    "    score=best_match[\"score\"]\n",
    "    description=entry[\"description\"]\n",
    "    print (entry[\"title\"])\n",
    "    print (description[0:min(len(description),150)])\n",
    "    print (score)\n",
    "    print (\"--------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
